# ComfyUI_HunyuanAvatar_Sm
* [HunyuanVideo-Avatar](https://github.com/Tencent-Hunyuan/HunyuanVideo-Avatar): High-Fidelity Audio-Driven Human Animation for Multiple Characters,try it in comfyUI ,if your VRAM >12 or 10G

TIPS:
-----
* é»‘å›¾çš„åŸå› å¯èƒ½æ˜¯æ˜¯diffuserç‰ˆæœ¬é™å®š33.0ï¼Œä»¥åŠprecisionæˆ‘å¼ºè¡Œæ”¹æˆfb16çš„ç¼˜æ•…ï¼Œç»ˆäºå¤ç°äº†ã€‚æµ‹è¯•ç¯å¢ƒï¼Œcu124ï¼Œtorch2.6  4070


1.Installation  
-----
In the ./ComfyUI /custom_node directory, run the following:   
```
git clone https://github.com/smthemex/ComfyUI_HunyuanAvatar_Sm.git
```  
  
2.requirements  
----
```
pip install -r requirements.txt
```

3 models 
----
* download files from [tencent/HunyuanVideo-Avatar](https://huggingface.co/tencent/HunyuanVideo-Avatar) 
```
â”œâ”€â”€ ComfyUI/models/HunyuanAvatar/
|   â”œâ”€â”€ det_align/
|         â”œâ”€â”€detface.pt
|   â”œâ”€â”€ llava_llama_image/
|         â”œâ”€â”€config.json
|         â”œâ”€â”€ ...æ‰€æœ‰jsonæ–‡ä»¶ä»¥åŠæ‰€æœ‰safetensorsæ¨¡å‹
|   â”œâ”€â”€text_encoder_2/
|         â”œâ”€â”€config.json
|         â”œâ”€â”€ ... æ‰€æœ‰jsonæ–‡ä»¶ä»¥åŠmodel.safetensorsæ¨¡å‹
|   â”œâ”€â”€vae/
|         â”œâ”€â”€config.json
|         â”œâ”€â”€ pytorch_model.pt
|   â”œâ”€â”€whisper-tiny/
|         â”œâ”€â”€config.json
|         â”œâ”€â”€ ... æ‰€æœ‰jsonæ–‡ä»¶ä»¥åŠmodel.safetensorsæ¨¡å‹
|   â”œâ”€â”€ mp_rank_00_model_states_fp8_map.pt #104K if use fp8  å¦‚æœç”¨fp8åˆ™ä¸‹è½½
|   â”œâ”€â”€ mp_rank_00_model_states_fp8.pt.pt #24.9G  if use fp8  å¦‚æœç”¨fp8åˆ™ä¸‹è½½
|   â”œâ”€â”€mp_rank_00_model_states.pt
```
4 example
----
![](https://github.com/smthemex/ComfyUI_HunyuanAvatar_Sm/blob/main/example_workflows/example.png)

## ğŸ”— BibTeX

If you find [HunyuanVideo-Avatar](https://arxiv.org/pdf/2505.20156) useful for your research and applications, please cite using this BibTeX:

```BibTeX
@misc{hu2025HunyuanVideo-Avatar,
      title={HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters}, 
      author={Yi Chen and Sen Liang and Zixiang Zhou and Ziyao Huang and Yifeng Ma and Junshu Tang and Qin Lin and Yuan Zhou and Qinglin Lu},
      year={2025},
      eprint={2505.20156},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/pdf/2505.20156}, 
}
```

## Acknowledgements

We would like to thank the contributors to the [HunyuanVideo](https://github.com/Tencent/HunyuanVideo), [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [FLUX](https://github.com/black-forest-labs/flux), [Llama](https://github.com/meta-llama/llama), [LLaVA](https://github.com/haotian-liu/LLaVA), [Xtuner](https://github.com/InternLM/xtuner), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research and exploration. 
